{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "la8I22c9PfMK"
      },
      "source": [
        "# Cosa sono veramente le **Neural Networks**? (parte II)\n",
        "\n",
        "In questa lezione proviamo ad **esplorare alcuni aspetti pratici che riguardano le reti neurali**: come implementarne (i) l'architettura, (ii) il meccanismo di addestramento, e (iii) cioè come usare architetture precedentemente create.\n",
        "\n",
        "Per farlo, ci serviremo di un esempio di classificazione intuitivo e di un insieme di dati reali."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V78ZeNBJwmXa"
      },
      "source": [
        "## Strumenti di lavoro\n",
        "\n",
        "Oltre al notebook per editare testo e codice, ci serviamo di una libreria per la manipolazione di un nuovo tipo di dato: il tensore.\n",
        "\n",
        "Inoltre, per questa lezione, ci ispiriamo ai seguenti materiali:\n",
        "1. [un tutorial ufficiale di `PyTorch`](https://pytorch.org/tutorials/beginner/nn_tutorial.html) firmato da _Jeremy Howard_ ([fast.ai](https://www.fast.ai)), da dove prendiamo in prestito il codice per la creazione di una rete neurale _from scratch_\n",
        "2. [un tutorial sulle basi di `PyTorch`](https://github.com/yunjey/pytorch-tutorial) dell'utente [@yunjey](https://github.com/yunjey)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AtymutIwn7p"
      },
      "source": [
        "## Prerequisiti \n",
        "\n",
        "Il tutorial assume la conoscenza dei seguenti concetti basilari: \n",
        "- **operazioni tra tensori** (vettori multidimensionali) che sono solitamente oggetto di un corso base di algebra lineare\n",
        "- qualche nozione di **teoria delle probabilità**\n",
        "- **un po' di Python** o di un linguaggio ad oggetti similare"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VM9JI0tuIMHp"
      },
      "source": [
        "# Elementi di `PyTorch`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZJ1U3u3INJf",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import torch # importiamo la libreria PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmz_NrM8qBdn"
      },
      "source": [
        "Per **manipolare tensori** e **definire le reti neurali** useremo il framework [PyTorch](https://pytorch.org/docs/stable) che mette a disposizione *classi*, *metodi* e *funzioni* che aiutano nella creazione e nell'addestramento delle reti neurali.\n",
        "\n",
        "I tensori di `PyTorch` hanno una proprietà molto interessate, **ci permettono di tenere traccia delle trasformazioni dell'input** - quelle che applichiamo al `torch.tensor`.\n",
        "`PyTorch` salva queste operazioni in uno spazio speciale nella struttura dati del `torch.tensor`.\n",
        "\n",
        "> Quello di cui abbiamo bisogno è un metodo per calcolare rapidamente la derivata della funzione che ha trasformato i nostri dati, assumendo che questa sia derivabile. \n",
        "\n",
        "> Vogliamo misurare il tasso di cambiamento della funzione di trasformazione rispetto alla/alle variabile/variabili di input.\n",
        "\n",
        "Per fare in modo che `PyTorch` tenga traccia delle operazioni di un `torch.tensor` fissiamo il parametro `requires_grad=True`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uF3vDelNo1DE",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# creiamo alcuni tensori\n",
        "x = torch.tensor(1., requires_grad=True) # un tensore di input x = [1.0]\n",
        "w = torch.tensor(2., requires_grad=True) # un tensore dei pesi w = [2.0]\n",
        "b = torch.tensor(3., requires_grad=True) # un tensore dei bias b = [3.0]\n",
        "\n",
        "# definiamo il modello del percettrone\n",
        "y = w * x + b # y = [5.0]\n",
        "\n",
        "# calcoliamo i gradienti\n",
        "y.backward(retain_graph=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoyTVtRNRKUL",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRCO8EiyNB6O"
      },
      "source": [
        "\n",
        "Per calcolare le derivate parziali $\\frac{\\partial y }{\\partial x}, \\frac{\\partial y }{\\partial w}, \\frac{\\partial y }{\\partial b}$ possiamo \"guardare\" nell'attributo `grad` del relativo `torch.tensor`.\n",
        "Certo, questo calcolo lo possiamo fare anche \"a mente\", giusto?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdRFerkxJM_2",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "print(x.grad)\n",
        "print(w.grad)\n",
        "print(b.grad) # ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JnA_lasRXdP"
      },
      "source": [
        "## Come è fatta una **Neural Network** in `PyTorch`\n",
        "\n",
        "Costruiamo una rete neurale completa servendoci dei tipi di `PyTorch`, ci limitiamo ad un semplice modello di regressione lineare."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7xo26IXJHLR",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "n_esempi = 10 # prendiamo 10 esempi\n",
        "n_variabili = 3 # ogni esempio è rappresentato da 3 variabili (larghezza, altezza, ampiezza)\n",
        "inputs = torch.randn(n_esempi, n_variabili) \n",
        "targets = torch.randn(n_esempi, 1) # dataset di train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJmq62i-TPvk",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "print(inputs[0], targets[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tc__VLTRC5c",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "neuroni_di_ingresso = 3\n",
        "neuroni_di_uscita = 1\n",
        "linear = torch.nn.Linear(neuroni_di_ingresso, neuroni_di_uscita) # il percettrone in una linea di codice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLCZrEF-WG9V",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "linear.weight # il tensore dei pesi embedded nel modello"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Svr43EzWM3t",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "linear.bias # il tensore dei bias embedded nel modello"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdCOQAduSnle",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "outputs = linear(inputs) # otteniamo le predizioni"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoggHa8cVbDB",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# guardiamo ad ogni target e relativa predizione\n",
        "for i in range(len(outputs)): \n",
        "  print(targets[i].data, outputs[i].data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tmdx6J3Ry0t7"
      },
      "source": [
        "### Come misuriamo la qualità della nostra predizione? \n",
        "\n",
        "Abbiamo bisogno di una quantità che ci dica come il nostro modello approssima il \"vero\" comportamento dei dati - ovvero, il _criterion_ del modello lineare.\n",
        "\n",
        "Scegliamo un _criterion_ semplice: il **Mean Squared Error** (MSE), che calcola la media delle distanze tra i dati di train e le predizioni. \n",
        "\n",
        "$$\n",
        "MSE = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat y_i)^2\n",
        "$$\n",
        "\n",
        "dove:\n",
        "- $N$ è il numero di `inputs`\n",
        "- $y_i$ è il valore `target` per l'`input` $i$-esimo\n",
        "- $\\hat y_i$ è il valore `output` per l'`input` $i$-esimo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLNUW1Zl2BLZ",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def MSE(N, y, hat_y):\n",
        "  mse = 0\n",
        "  for i in range(N):\n",
        "    mse += (y[i] - hat_y[i])**2\n",
        "  return 1/N * mse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eAsSDDU6UWU"
      },
      "source": [
        "Il calcolo della _loss_ può essere fatto con la nostra funzione `MSE` ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9Gea3Tr3r0c",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "loss = MSE(n_esempi, outputs, targets)\n",
        "loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzXz96xv6c9o"
      },
      "source": [
        "... oppure con la funzione che ci fornisce `PyTorch` ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QPLq-GfSbvo",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.MSELoss()\n",
        "loss = criterion(outputs, targets) # chiamo la funzione di calcolo della loss\n",
        "loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q34q1ucF7weV"
      },
      "source": [
        "Per effettuare l'aggiornamento dei pesi e dei bias calcoliamo i gradienti dei tensori con il metodo `backward()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qb-lj5oiSqOw",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "loss.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bT-XBPhnSrWY",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "linear.weight.grad # il gradiente dei pesi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mOSwynb8Cb0",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "linear.bias.grad # il gradiente dei bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Vme-a9TThow"
      },
      "source": [
        "Aggiorniamo il tensore dei pesi sottraendo il gradiente appena calcolato, ricordiamo di tenere in conto anche un oppportuno valore per il `learning rate` (diciamo $0.01$).\n",
        "\n",
        "$$\n",
        "w_i = w_i - ( \\frac{\\partial L}{\\partial w} * lr ) \\qquad \\forall w_i \\in w\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoBxdNiOSsX2",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "w = linear.weight.data\n",
        "print(w)\n",
        "w_grad = linear.weight.grad.data\n",
        "for i in range(len(w)):\n",
        "  w[i] -= w_grad[i] * 0.01\n",
        "print(w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqBHompE8r-a"
      },
      "source": [
        "Lo stesso calcolo va fatto per il tensore dei bias.\n",
        "\n",
        "$$\n",
        "b_i = b_i - ( \\frac{\\partial L}{\\partial b} * lr ) \\qquad \\forall b_i \\in b\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUO1Dzgz8vJT",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "b = linear.bias.data\n",
        "print(b)\n",
        "b_grad = linear.bias.grad.data\n",
        "for i in range(len(b)):\n",
        "  b[i] -= b_grad[i] * 0.01\n",
        "print(b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPPzfw0idRMm"
      },
      "source": [
        "**Oppure** possiamo usare (come nel caso del _criterion_) la funzione `SGD` messa a disposizione da `PyTorch`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V126ae7LUQ3u",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "print(linear.weight.data)\n",
        "print(linear.bias.data)\n",
        "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n",
        "optimizer.step()\n",
        "print(linear.weight.data)\n",
        "print(linear.bias.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Os1WB_DIT9m2"
      },
      "source": [
        "In alternativa possiamo definire un `optimizer`, un tipo di `PyTorch` che esegue esattamente lo stesso comportamento appena implementato.\n",
        "\n",
        "> I tensori di pesi e bias prendono - insieme - il nome di **parametri della rete**. \n",
        "Sono tutti quei valori il cui aggiornamento comporta l'**apprendimento** del modello.\n",
        "\n",
        "Nel gergo delle reti esiste il \"numero di parametri apprendibili della rete\" = somma delle dimensioni del _tensore_ di **pesi** e del _tensore_ di **bias**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytFDF_8-eG3j",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "sum([p.numel() for p in linear.parameters()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZkcztkGKjRM"
      },
      "source": [
        "# [MNIST](https://en.wikipedia.org/wiki/MNIST_database) $-$ _handwritten digits classification_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rp937dLrKjsg",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28JSAbc7z36F"
      },
      "source": [
        "![MNIST example](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)\n",
        "\n",
        "Il problema che vogliamo risolvere è classificare correttamente il valore rappresentato nella fotografia di una cifra scritta a mano ([clicca qui se vuoi saperne di più](http://yann.lecun.com/exdb/mnist/)).\n",
        "\n",
        "Ma prima pensiamo ad una alternativa, come potremmo rappresentare questo problema con un sistema di _decision rules_?\n",
        "\n",
        "> *Riusciamo a trovare una congiunzione/disgiunzione di relazioni tra attributi e valori per la prima fotografia in alto a sinistra nell'immagine? Come la  distinguiamo da quella in basso a destra?*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "au1FbhejKWgJ",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "train_dataset = torchvision.datasets.MNIST(root='sample_data',\n",
        "                                           train=True, \n",
        "                                           transform=torchvision.transforms.ToTensor(),  \n",
        "                                           download=True)\n",
        "\n",
        "val_dataset = torchvision.datasets.MNIST(root='sample_data', \n",
        "                                          train=False, \n",
        "                                          transform=torchvision.transforms.ToTensor())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Sjg1CLzt91X",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ct9h1HDIiev-",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "val_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnrWbuK-8veg"
      },
      "source": [
        "Diamo un occhio ad un file di esempio per vedere come è rappresentato l'input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNe5D467L2MI",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "scegli_un_numero = 214124 % len(train_dataset)\n",
        "immagine, classe = train_dataset[scegli_un_numero] # il dataset è fatto di tuple (immagine, classe)\n",
        "\n",
        "size = (28,28) # decidiamo come vogliamo \"vedere\" l'input\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(immagine.reshape(size)) #, cmap=\"gray\")\n",
        "plt.show()\n",
        "\n",
        "# print(immagine)\n",
        "print(classe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxfOnadf8fpr"
      },
      "source": [
        "## Tutto insieme in un esempio con `PyTorch`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFVcfp19yrBJ"
      },
      "source": [
        "Inizializziamo gli iperparametri per l'addestramento, potrebbe essere possibili provare di verse combinazioni per questi valori prima di incontrare la configurazione migliore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3mcj8UYyqZs",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-4 # potenze negative del 10 descrescente\n",
        "batch_size = 64 # il numero di input che vengono processati dal modello (progessione in potenze del 2 crescenti)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeoWDpgmyIsi",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "n_pixels = 28*28 \n",
        "n_variabili = n_pixels # ?\n",
        "n_classi = 10 # ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLk1Rppuz0Jx",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "model = torch.nn.Linear(n_variabili, n_classi)\n",
        "sum([p.numel() for p in model.parameters()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEr3R0oxmewD",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# confrontiamo con il nostro calcolo del numero di parametri \n",
        "28 * 28 * 10 + 10 #larghezza * altezza * n_classi + bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6hi1ym9wHiR",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# modello alternativo un po' più complesso\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(n_variabili,n_variabili//2),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(n_variabili//2, n_classi)\n",
        ")\n",
        "sum([p.numel() for p in model.parameters()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vmOO6styDYs",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CKE-8Wv_Qki"
      },
      "source": [
        "Il _criterion_ che scegliamo è il **Negative Log Likelihood** (NLL), utile quando le classi del nostro problema sono **valori** (al plurale) discreti.\n",
        "Si tratta di una funzione di costo che ci dice quanto \"male\" il modello sta funzionando: \n",
        "\n",
        "> _the lower, the better_.\n",
        "\n",
        "**Ma** NLL ha bisogno di un input espresso come il logaritmo di una distribuzione di probabilità (la _log probability_, di cui calcolare la _negative likelihood_. \n",
        "\n",
        "Per saperne qualcosa di più potete leggere [questo post su Medium](https://medium.com/deeplearningmadeeasy/negative-log-likelihood-6bd79b55d8b6) e [la pagina relativa alla Cross entropy di Wikipedia](https://en.wikipedia.org/wiki/Cross_entropy).\n",
        "\n",
        "Per trasformare l'input in una distribuzione di probabilità ci serviamo della funzione `log softmax`:\n",
        "\n",
        "$$\n",
        "f(x_{i}) = \\log\\left(\\frac{{\\rm e}^{x_i}}{ \\sum_j {\\rm e}^{x_j}}\\right)\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXW6J1-I-pYN",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "log_softmax = torch.nn.LogSoftmax(dim=1)\n",
        "criterion = torch.nn.NLLLoss() # Negative Log Likelihood  \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epNppyu-JMhn",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "max_epoch = 10\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(max_epoch):\n",
        "  for i, (immagini, classi) in enumerate(train_loader):\n",
        "    inputs = immagini.reshape(-1, n_variabili)\n",
        "\n",
        "    outputs = model(inputs)\n",
        "    log_probs = log_softmax(outputs)\n",
        "    loss = criterion(log_probs, classi)\n",
        "    \n",
        "    optimizer.zero_grad() # resetto l'aggiornamento\n",
        "    loss.backward() # calcolo i gradienti\n",
        "    optimizer.step() # aggiorno i parametri\n",
        "        \n",
        "    if (i+1) % 100 == 0:\n",
        "      print(f'Epoca [{epoch+1}/{max_epoch}], Step [{i+1}/{total_step}], Loss: {loss:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XspvkZ5b0BsW"
      },
      "source": [
        "Testiamo su un insieme di dati mai visti durante il training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BKGEL7Jz_6O",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "corrette = 0\n",
        "totali = 0\n",
        "for immagini, classi in val_loader:\n",
        "  inputs = immagini.reshape(-1, n_variabili)\n",
        "  outputs = model(inputs)\n",
        "  _, pred = torch.max(outputs.data, 1)\n",
        "  totali += classi.size(0)\n",
        "  corrette += (pred == classi).sum()\n",
        "\n",
        "print(f'Accuracy of the model on 10000 validation images: {100 * corrette // totali} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yL1BKPEwJSW1"
      },
      "source": [
        "# Grazie per la vostra attenzione :-)\n",
        "\n",
        "Mi potete contattare alla mail istituzionali [stefanopio \\[dot\\] zingaro \\[at\\] unibo \\[dot\\] it](mailto:stefanopio.zingaro@unibo.it)\n",
        "\n",
        "[Attribuzione Internazionale Creative Commons 4.0](http://creativecommons.org/licenses/by-sa/4.0/)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "mnist.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
