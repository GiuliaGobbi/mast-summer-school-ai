{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have created the spectrogram images it's time to build the computer vision model. If you are following along with the learning path then you already created a computer vision model in the second module in this path. We will be creating a computer vision model -- they may look familiar! Like always we first import the packages we need to build the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow_io as tfio\n",
        "import IPython.display as ipd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Spectrogram images into a dataset for training\n",
        "\n",
        "Here we provide the path to our image data and use [tf.keras.preprocessing.image_dataset_from_directory](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory) to load the images into tensors. \n",
        "\n",
        "This method is doing a lot for us. Lets take a look at a few of the params:\n",
        "- `labels='inferred'`: The labels are created based on folder directory names.\n",
        "- `image_size=(256, 256)`: resizes the image\n",
        "- `validation_split=0.2, subset='validation'`: create validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_directory = './data/train/'\n",
        "test_directory = './data/test/'\n",
        "\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    train_directory, labels='inferred', label_mode='int', image_size=(256, 256), seed=123, \n",
        "    validation_split=0.2, subset='validation')\n",
        "\n",
        "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    test_directory, labels='inferred', label_mode='int', image_size=(256, 256), \n",
        "    validation_split=None, subset=None)\n",
        "\n",
        "class_names = train_ds.class_names\n",
        "print(class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Display spectrogram images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_ds.take(1):\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "        plt.title(class_names[labels[i]])\n",
        "        plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create the model\n",
        "\n",
        "We are ready to create the Convolution Neural Network for the computer vision model to process the spectogram images.\n",
        "\n",
        "To contruct the linear layers we use the [tf.keras.Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) and pass in a list with each layer. Read more about the layers [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_classes = 2\n",
        "img_height = 256\n",
        "img_width = 256\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
        "  tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "  tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "  tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(num_classes)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Set the `learning_rate`, loss function `loss_fn`, `optimizer` and `metrics`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_rate = 0.125\n",
        "\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
        "metrics = ['accuracy']\n",
        "model.compile(optimizer, loss_fn, metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set the epocks\n",
        "epochs = 15\n",
        "print('\\nFitting:')\n",
        "\n",
        "# Train the model.\n",
        "history = model.fit(train_ds, epochs=epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ## Test the model\n",
        " \n",
        "Awesome! You should have got somewhere between a 93%-95% accuracy by the 15th epoch. Here we grab a batch from our test data and see how the model performs on the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "correct = 0\n",
        "batch_size = 0\n",
        "for batch_num, (X, Y) in enumerate(test_ds):\n",
        "    batch_size = len(Y)\n",
        "    pred = model.predict(X)\n",
        "    for i in range(batch_size):\n",
        "        predicted = np.argmax(pred[i], axis=-1)\n",
        "        actual = Y[i]\n",
        "        #print(f'predicted {predicted}, actual {actual}')\n",
        "        if predicted == actual:\n",
        "            correct += 1\n",
        "    break\n",
        "\n",
        "print(f'Number correct: {correct} out of {batch_size}')\n",
        "print(f'Accuracy {correct / batch_size}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced Optional Step: Test your voice\n",
        "\n",
        "This optional section allows you to use an audio file of your own voice saying \"yes\" or \"no\" and see if the model predicts correctly.  Since it requires you to add your own audio file, you will need to have the ability to record your voice to a wave format and put it in a publically accesible place. The example code has a local directory -- make sure you update the `./data/myvoice/yes.wav` and `./data/myvoice/no.wav` to your data path.\n",
        "\n",
        "First we'll create a `load_audio` function where we set the `sample_rate` to `16000` and the audio `channels` to `1`, load the file, and use `tfio.audio.resample` to resample the audio to the desired rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_audio(file_path):\n",
        "    sample_rate = 16000\n",
        "    channels = 1\n",
        "    audio_binary = tf.io.read_file(file_path)\n",
        "    audio, original_sample_rate = tf.audio.decode_wav(audio_binary) # desired_samples=sample_rate,desired_channels=channels \n",
        "    audio = tfio.audio.resample(audio, original_sample_rate.numpy(), sample_rate)\n",
        "    waveform = tf.squeeze(audio, axis=-1)\n",
        "    return waveform, sample_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "no_waveform, no_sample_rate = load_audio('./data/myvoice/no.wav')\n",
        "yes_waveform, yes_sample_rate = load_audio('./data/myvoice/yes.wav')\n",
        "\n",
        "print(no_waveform.shape)\n",
        "print(yes_waveform.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Listen to the audio files now that they have been loaded and resampled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ipd.Audio(no_waveform.numpy(), rate=16000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ipd.Audio(yes_waveform.numpy(), rate=16000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Create a funciton to `get_spectrogram`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_spectrogram(waveform):\n",
        "    # Padding for files with less than 16000 samples\n",
        "    zero_padding = tf.zeros([16000] - tf.shape(waveform), dtype=tf.float32)\n",
        "\n",
        "    # Concatenate audio with padding so that all audio clips will be of the same length\n",
        "    waveform = tf.cast(waveform, tf.float32)\n",
        "    equal_length = tf.concat([waveform, zero_padding], 0)\n",
        "    print(len(equal_length))\n",
        "    \n",
        "    # Create spectrogram with tfio\n",
        "    spect = tfio.audio.spectrogram(input=equal_length, nfft=255, window=255, stride=128)\n",
        "    \n",
        "    # Or create with tf.signal\n",
        "    spectrogram = tf.signal.stft(equal_length, frame_length=255, frame_step=128)\n",
        "    spectrogram = tf.abs(spectrogram)\n",
        "\n",
        "    return spectrogram, spect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "no_spectrogram, no_spect = get_spectrogram(no_waveform)\n",
        "yes_spectrogram, yes_spect = get_spectrogram(yes_waveform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Create method `plot_spectrogram` to view your voice spectrogram."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_spectrogram(spectrogram, ax, title):\n",
        "    # Convert to frequencies to log scale and transpose so that the time is\n",
        "    # represented in the x-axis (columns).\n",
        "    log_spec = np.log(spectrogram.T)\n",
        "    height = log_spec.shape[0]\n",
        "    width = log_spec.shape[1]\n",
        "    X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n",
        "    Y = range(height)\n",
        "    ax.pcolormesh(X, Y, log_spec)\n",
        "    ax.set_xlim([0, 16000])\n",
        "    ax.set_title(title)\n",
        "    \n",
        "fig, ax = plt.subplots()  \n",
        "plot_spectrogram(no_spectrogram.numpy(), ax, 'No Spectrogram')\n",
        "plt.show()\n",
        "\n",
        "fig, ax = plt.subplots()  \n",
        "plot_spectrogram(yes_spectrogram.numpy(), ax, 'Yes Spectrogram')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Save image to local path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "no_path = './data/myvoice/no_myspec_img.png'\n",
        "yes_path = './data/myvoice/yes_myspec_img.png'\n",
        "\n",
        "plt.imsave(no_path, no_spectrogram.numpy(), cmap='gray')\n",
        "plt.imsave(yes_path, yes_spectrogram.numpy(), cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Load image to PIL format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "yes_pil_img = tf.keras.preprocessing.image.load_img(\n",
        "    yes_path, grayscale=False, color_mode='rgb', target_size=[img_height,img_width],\n",
        "    interpolation='nearest'\n",
        ")\n",
        "\n",
        "no_pil_img = tf.keras.preprocessing.image.load_img(\n",
        "    no_path, grayscale=False, color_mode='rgb', target_size=[img_height,img_width],\n",
        "    interpolation='nearest'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "yes_pil_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "no_pil_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "yes_img_tensor = np.array(yes_pil_img)\n",
        "no_img_tensor = np.array(no_pil_img)\n",
        "print(yes_img_tensor.shape)\n",
        "print(no_img_tensor.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Now use the model to predict on your voice!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "yes_pred = model.predict(yes_img_tensor[None,:,:])\n",
        "no_pred = model.predict(no_img_tensor[None,:,:])\n",
        "\n",
        "yes_predicted = np.argmax(yes_pred)\n",
        "yes_string_result = 'yes' if yes_predicted == 1 else 'no'\n",
        "\n",
        "no_predicted = np.argmax(no_pred)\n",
        "no_string_result = 'yes' if no_predicted == 1 else 'no'\n",
        "\n",
        "print(f'Prediction for yes is {yes_string_result}')\n",
        "print(f'Prediction for no is {no_string_result}')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
