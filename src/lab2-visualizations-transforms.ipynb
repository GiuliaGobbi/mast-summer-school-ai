{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data visualization and transformation is an important part of every model. Now that we have our dataset downloaded, let's learn more about audio data visualization and transforming this dataset.\n",
        "\n",
        "TorchAudio has many transformation functions for audio manipulation and feature extractions. However, in this module, your focus is on the following concepts and transforms: `Spectrogram`, `MelSpectrogram`, `Waveform`, and `MFCC`. After you understand these concepts, you'll create your spectrogram images of the yes/no dataset to be used in the computer vision model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **Spectrogram**: Create a spectrogram from a waveform.\n",
        "- **MFCC**: Create the Mel-frequency cepstrum coefficients from a waveform.\n",
        "- **MelSpectrogram**: Create Mel spectrograms from a waveform by using the `STFT` function in PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the dataset folders into a data loader\n",
        "\n",
        "Import the packages, and create a `load_audio_files` function to load audio files from a specified path into a dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we'll go through the audio file that we downloaded in our local directory by filtering out the ones that are the `yes` and `no` commands under the _nohash_ path. Then we'll load the files into the `torchaudio` data object. This will make it easy to extract the attributes of the audio (for example, the waveform and sample rate)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "json"
        }
      },
      "outputs": [],
      "source": [
        "default_dir = os.getcwd()\n",
        "folder = 'data'\n",
        "\n",
        "def load_audio_files(path: str, label:str):\n",
        "\n",
        "    dataset = []\n",
        "    walker = sorted(str(p) for p in Path(path).glob(f'*.wav'))\n",
        "\n",
        "    for i, file_path in enumerate(walker):\n",
        "        path, filename = os.path.split(file_path)\n",
        "        speaker, _ = os.path.splitext(filename)\n",
        "        speaker_id, utterance_number = speaker.split(\"_nohash_\")\n",
        "        utterance_number = int(utterance_number)\n",
        "    \n",
        "        # Load audio\n",
        "        waveform, sample_rate = torchaudio.load(file_path)\n",
        "        dataset.append([waveform, sample_rate, label, speaker_id, utterance_number])\n",
        "        \n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Call the `load_audio_files` function to load the contents from each of the audio class files, as well as their metadata.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainset_speechcommands_yes = load_audio_files(f'./{folder}/SpeechCommands/speech_commands_v0.02/yes', 'yes')\n",
        "trainset_speechcommands_no = load_audio_files(f'./{folder}/SpeechCommands/speech_commands_v0.02/no', 'no')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print(f'Length of yes dataset: {len(trainset_speechcommands_yes)}')\n",
        "print(f'Length of no dataset: {len(trainset_speechcommands_no)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now load the dataset into a data loader for both `yes` and `no` training sample sets. `DataLoader` sets the number of batches you want to iterate to load the dataset through your network, to train the model. We'll set the batch size to 1, because we want to load the entire batch in one iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainloader_yes = torch.utils.data.DataLoader(trainset_speechcommands_yes, batch_size=1,\n",
        "                                            shuffle=True, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainloader_no = torch.utils.data.DataLoader(trainset_speechcommands_no, batch_size=1,\n",
        "                                            shuffle=True, num_workers=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To see how the data looks, we'll grab the waveform and sample rate from each class, and print out a sample of the dataset.\n",
        "\n",
        "- The **waveform** value is in a Tensor with a float datatype.\n",
        "- The **sample rate** value is 16000 in the format the audio signal was captured.\n",
        "- The **label** value is the command classification of the word uttered in the audio, `yes` or `no`.\n",
        "- The **ID** is a unique identifier of the audio file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "yes_waveform = trainset_speechcommands_yes[0][0]\n",
        "yes_sample_rate = trainset_speechcommands_yes[0][1]\n",
        "print(f'Yes Waveform: {yes_waveform}')\n",
        "print(f'Yes Sample Rate: {yes_sample_rate}')\n",
        "print(f'Yes Label: {trainset_speechcommands_yes[0][2]}')\n",
        "print(f'Yes ID: {trainset_speechcommands_yes[0][3]} \\n')\n",
        "\n",
        "no_waveform = trainset_speechcommands_no[0][0]\n",
        "no_sample_rate = trainset_speechcommands_no[0][1]\n",
        "print(f'No Waveform: {no_waveform}')\n",
        "print(f'No Sample Rate: {no_sample_rate}')\n",
        "print(f'No Label: {trainset_speechcommands_no[0][2]}')\n",
        "print(f'No ID: {trainset_speechcommands_no[0][3]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transform and visualize\n",
        "\n",
        "Our data is ready! Let's break down some of the audio transforms and the visualization to better understand what they are, and what they tell us about the data.\n",
        "\n",
        "### Waveform\n",
        "\n",
        "The waveform is generated by taking the sample rate and frequency, and representing the signal visually. This signal can be represented as a `waveform`, which is the `signal` representation over time, in a graphical format. The audio can be recorded in different `channels`. For example, stereo recordings have two channels: right and left.\n",
        "\n",
        "Here's how to use the `resample` transform to reduce the size of the waveform, and then graph the data to visualize the new waveform shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_waveform(waveform, sample_rate, label):\n",
        "    print(\"Waveform: {}\\nSample rate: {}\\nLabels: {} \\n\".format(waveform, sample_rate, label))\n",
        "    new_sample_rate = sample_rate/10\n",
        "   \n",
        "    # Resample applies to a single channel, we resample first channel here\n",
        "    channel = 0\n",
        "    waveform_transformed = torchaudio.transforms.Resample(sample_rate, new_sample_rate)(waveform[channel,:].view(1,-1))\n",
        "\n",
        "    print(\"Shape of transformed waveform: {}\\nSample rate: {}\".format(waveform_transformed.size(), new_sample_rate))\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(waveform_transformed[0,:].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The displayed results show how the sample rate is transformed from 16000 to 1600."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "show_waveform(yes_waveform, yes_sample_rate, 'yes')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "show_waveform(no_waveform, no_sample_rate, 'no')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Spectrogram\n",
        "\n",
        "So, what is a spectrogram? A spectrogram maps the frequency to time of an audio file, and it allows you to visualize audio data by frequency. It's in an image format. This image is what we'll use for our computer vision classification on the audio files. You can view the spectrogram image in grayscale, or in Red Green Blue (RGB) color format.\n",
        "\n",
        "Every spectrogram image helps show the different features the sound signal produces in a color pattern. The convolutional neural network (CNN) treats the color patterns in the image as features for training the model to classify the audio.\n",
        "\n",
        "Let's use the PyTorch `torchaudio.transforms` function to transform the waveform to a spectrogram image format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_spectrogram(waveform_classA, waveform_classB):\n",
        "    yes_spectrogram = torchaudio.transforms.Spectrogram()(waveform_classA)\n",
        "    print(\"\\nShape of yes spectrogram: {}\".format(yes_spectrogram.size()))\n",
        "    \n",
        "    no_spectrogram = torchaudio.transforms.Spectrogram()(waveform_classB)\n",
        "    print(\"Shape of no spectrogram: {}\".format(no_spectrogram.size()))\n",
        "\n",
        "    plt.figure()\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.title(\"Features of {}\".format('no'))\n",
        "    plt.imshow(yes_spectrogram.log2()[0,:,:].numpy(), cmap='viridis')\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.title(\"Features of {}\".format('yes'))\n",
        "    plt.imshow(no_spectrogram.log2()[0,:,:].numpy(), cmap='viridis')  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll use the waveform for the `yes` command to display the spectrogram images dimensions and color pattern in an RGB chart. We'll also compare the feature difference between the `yes` and `no` audio commands.\n",
        "\n",
        "- The **y-axis** is the frequency of the audio.\n",
        "- The **x-axis** is the time of the audio.\n",
        "- The intensity of the image shows the amplitude or pitch of the audio. In the following spectrogram images, the high concentrate of the yellow color illustrates the amplitude of the audio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "show_spectrogram(yes_waveform, no_waveform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mel spectrogram\n",
        "\n",
        "Mel spectrogram is also a frequency to time, but the frequency is converted to the Mel scale. The Mel scale takes the frequency and changes it, based on the perception of the sound of the scale or melody. This transforms the frequency within to the Mel scale, and then creates the spectrogram image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_melspectrogram(waveform,sample_rate):\n",
        "    mel_spectrogram = torchaudio.transforms.MelSpectrogram(sample_rate)(waveform)\n",
        "    print(\"Shape of spectrogram: {}\".format(mel_spectrogram.size()))\n",
        "\n",
        "    plt.figure()\n",
        "    plt.imshow(mel_spectrogram.log2()[0,:,:].numpy(), cmap='viridis')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "show_melspectrogram(yes_waveform, yes_sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mel-frequency cepstral coefficients (MFCC)\n",
        "\n",
        "A simplified explanation of what the MFCC does is that it takes our frequency, applies transforms, and the result is the amplitudes of the spectrum created from the frequency. Let's take a look at what this looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_mfcc(waveform,sample_rate):\n",
        "    mfcc_spectrogram = torchaudio.transforms.MFCC(sample_rate= sample_rate)(waveform)\n",
        "    print(\"Shape of spectrogram: {}\".format(mfcc_spectrogram.size()))\n",
        "\n",
        "    plt.figure()\n",
        "    fig1 = plt.gcf()\n",
        "    plt.imshow(mfcc_spectrogram.log2()[0,:,:].numpy(), cmap='viridis')\n",
        "    \n",
        "    plt.figure()\n",
        "    plt.plot(mfcc_spectrogram.log2()[0,:,:].numpy())\n",
        "    plt.draw()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "show_mfcc(no_waveform,  no_sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create an image from a spectrogram\n",
        "\n",
        "At this point, you have a better understanding of your audio data, and different transformations you can use on it. Now, let's create the images we will use for classification. \n",
        "\n",
        "The following are two different functions to create the spectrogram image or the MFCC images for classification. You'll use the spectrogram images to train our model. Feel free to play around with MFCC classification on your own, if you want to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_spectrogram_images(trainloader, label_dir):\n",
        "    #make directory\n",
        "    directory = f'./{folder}/spectrograms/{label_dir}/'\n",
        "    if(os.path.isdir(directory)):\n",
        "        print(\"Data exists for\", label_dir)\n",
        "    else:\n",
        "        os.makedirs(directory, mode=0o777, exist_ok=True)\n",
        "        \n",
        "        for i, data in enumerate(trainloader):\n",
        "\n",
        "            waveform = data[0]\n",
        "            sample_rate = data[1][0]\n",
        "            label = data[2]\n",
        "            ID = data[3]\n",
        "\n",
        "            # create transformed waveforms\n",
        "            spectrogram_tensor = torchaudio.transforms.Spectrogram()(waveform)     \n",
        "            \n",
        "            fig = plt.figure()\n",
        "            plt.imsave(f'./{folder}/spectrograms/{label_dir}/spec_img{i}.png', spectrogram_tensor[0].log2()[0,:,:].numpy(), cmap='viridis')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here's the `define` function to create the MFCC images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_mfcc_images(trainloader, label_dir):\n",
        "    #make directory\n",
        "    os.makedirs(f'./{folder}/mfcc_spectrograms/{label_dir}/', mode=0o777, exist_ok=True)\n",
        "    \n",
        "    for i, data in enumerate(trainloader):\n",
        "\n",
        "        waveform = data[0]\n",
        "        sample_rate = data[1][0]\n",
        "        label = data[2]\n",
        "        ID = data[3]\n",
        "        \n",
        "        mfcc_spectrogram = torchaudio.transforms.MFCC(sample_rate= sample_rate)(waveform)\n",
        "\n",
        "        plt.figure()\n",
        "        fig1 = plt.gcf()\n",
        "        plt.imshow(mfcc_spectrogram[0].log2()[0,:,:].numpy(), cmap='viridis')\n",
        "        plt.draw()\n",
        "        fig1.savefig(f'./{folder}/mfcc_spectrograms/{label_dir}/spec_img{i}.png', dpi=100)\n",
        " \n",
        "        #spectorgram_train.append([spectrogram_tensor, label, sample_rate, ID])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create the spectrogram images that you'll use for the audio classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "create_spectrogram_images(trainloader_yes, 'yes')\n",
        "create_spectrogram_images(trainloader_no, 'no')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now have our audio as spectrogram images and are ready to build the model!"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "azureml_py38_PT_and_TF"
    },
    "kernelspec": {
      "display_name": "azureml_py38_PT_and_TF",
      "language": "python",
      "name": "conda-env-azureml_py38_PT_and_TF-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
