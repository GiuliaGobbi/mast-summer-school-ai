{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need to first get a better understanding of our audio data. Here we will look at some key concepts and features of audio data.\n",
        "\n",
        "Let's think about the digital representation of analog sound. How does sound get recorded anyway?! Just like with images we need to take our physical world and convert it to numbers or a digital representation for a computer to understand. For audio, a microphone is used to capture the sound and then it's converted from analog sound to digital sound by sampling at consistent intervals of time. This is called the `sample rate`. The higher the `sample rate` the higher the quality of the sound; however after a certain point the difference is not able to be detected by the human ear. The average sound sample rate is 48 kHz or 48,000 samples per second. The dataset we'll use in this module was sampled at 16kHz, so our data was sampled 16,000 times per second.\n",
        "\n",
        "When we sample audio, we measure the `amplitude` or how loud the audio is, at a particular sample rate. We can then take that information and represent the `signal` in graphical format as a `waveform`. Another way to look at the audio information is by analyzing its `frequency` or pitch, which represents the number of complete waves per time period -- the more waves, the higher the pitch! Also, audio can be recorded in different `channels`. For example stereo recordings have 2 channels, right and left.\n",
        "\n",
        "Now that we understand a bit about how we get our audio file, let's take a moment to understand how we might want to parse out a file. If you have longer audio files, you may want to split it out into `frames` or sections of the audio to be classified individually. For this dataset we don't need to split the data into frames because each sample is only one second and one word. Another processing step might be an `offset` which means the number of frames from the start of the file to begin data loading.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import\n",
        "\n",
        "Lets get started! First we will import the packages needed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import IPython.display as ipd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get the speech commands dataset\n",
        "\n",
        "We will use a tensorflow dataset called [Speech Commands](https://www.tensorflow.org/datasets/catalog/speech_commands). We will download the dataset but we are going to only use the `yes` and `no` classes to create a binary classification model.\n",
        "\n",
        "Now we extract the data from a local data path:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This method is used for the local dataset that has already been downloaded to the Learn enviornment.\n",
        "tf.keras.utils.get_file(\n",
        "     'speech_commands_v0.02.tar.gz',\n",
        "      origin = \"./data/speech_commands_v0.02.tar.gz\",\n",
        "      extract=True,\n",
        "      cache_dir='.', cache_subdir='data')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you are going to run this notebook locally on your own machine, use the below code to download a smaller subset of the data from googe storage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## This method is for local development to download from the public storage location.\n",
        "#\n",
        "# data_dir = pathlib.Path('./data/mini_speech_commands')\n",
        "# if not data_dir.exists():\n",
        "#   tf.keras.utils.get_file(\n",
        "#       'mini_speech_commands.zip',\n",
        "#       origin=\"http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip\",\n",
        "#       extract=True,\n",
        "#       cache_dir='.', cache_subdir='data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Change directory to downloaded data\n",
        "os.chdir('./data')\n",
        "default_dir = os.getcwd()\n",
        "print(f'Data directory will be: {default_dir}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_audio(file_path):\n",
        "    audio_binary = tf.io.read_file(file_path)\n",
        "    audio, sample_rate = tf.audio.decode_wav(audio_binary)\n",
        "    waveform = tf.squeeze(audio, axis=-1)\n",
        "    return waveform, sample_rate\n",
        "\n",
        "def load_audio_files(path: str, label:str):\n",
        "\n",
        "    dataset = []\n",
        "    walker = sorted(str(p) for p in Path(path).glob(f'*.wav'))\n",
        "\n",
        "    for i, file_path in enumerate(walker):\n",
        "        path, filename = os.path.split(file_path)\n",
        "        speaker, _ = os.path.splitext(filename)\n",
        "        speaker_id, utterance_number = speaker.split(\"_nohash_\")\n",
        "        utterance_number = int(utterance_number)\n",
        "    \n",
        "        # Load audio\n",
        "        waveform, sample_rate = load_audio(file_path)\n",
        "        dataset.append([waveform, sample_rate, label, speaker_id, utterance_number])\n",
        "        \n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Visualize the classes available in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "labels = [name for name in os.listdir('.') if os.path.isdir(name)]\n",
        "# back to default directory\n",
        "os.chdir(default_dir)\n",
        "print(f'Total Labels: {len(labels)}')\n",
        "print(f'Label Names: {labels}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "filenames = tf.io.gfile.glob(str(default_dir) + '/*/*')\n",
        "num_samples = len(filenames)\n",
        "print('Number of total examples:', num_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Convert the sound to tensor\n",
        "\n",
        "You likely have used a wave file before and understand that this is one format in which we save our digital representation of our analog audio to be shared and played. The Speech Commands dataset that we are using for this tutorial is stored in wave files that are all one second or less.\n",
        "\n",
        "Let's load up one of the wave files and take a look at how the tensor for the `waveform` looks. We will do this by creating and calling a `load_audio` function that will read the wave audio file and return the `waveform` tensor and `sample_rate`. The function gets the `audio_binary` with [tf.io.read_file](https://www.tensorflow.org/api_docs/python/tf/io/read_file) then uses [tf.audio.decode_wav](https://www.tensorflow.org/api_docs/python/tf/audio/decode_wav) to get the audio `waveform` to a tensor format (range of: -1 to 1) and the `sample_rate`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_path = \"./yes/00f0204f_nohash_0.wav\"\n",
        "\n",
        "def load_audio(file_path):\n",
        "    audio_binary = tf.io.read_file(file_path)\n",
        "    audio, sample_rate = tf.audio.decode_wav(audio_binary)\n",
        "    waveform = tf.squeeze(audio, axis=-1)\n",
        "    return waveform, sample_rate.numpy()\n",
        "\n",
        "waveform, sample_rate = load_audio(file_path)\n",
        "print('Waveform Tensor:', waveform)\n",
        "print('Sample Rate:', sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot the waveform\n",
        "\n",
        "Below we will create a `plot_audio` function to display the waveform and listen to a sample of each class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_audio(filename):\n",
        "    waveform, sample_rate = load_audio(filename)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(waveform.numpy())\n",
        "\n",
        "    return waveform, sample_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "filename = \"./yes/00f0204f_nohash_0.wav\"\n",
        "waveform, sample_rate = plot_audio(filename)\n",
        "ipd.Audio(waveform.numpy(), rate=sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "filename = \"./no/0c2ca723_nohash_0.wav\"\n",
        "waveform, sample_rate = plot_audio(filename)\n",
        "ipd.Audio(waveform.numpy(), rate=sample_rate)"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "azureml_py38_PT_and_TF"
    },
    "kernelspec": {
      "display_name": "Python 3.10.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
