{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data visualization and transformation is an important part of every model. Now that we have our dataset downloaded, let's learn more about audio data visualization and transforming this dataset. We will be using [tf.signal](https://www.tensorflow.org/api_docs/python/tf/signal) and [tfio.audio](https://www.tensorflow.org/io/api_docs/python/tfio/audio/spectrogram) processing operations to transform the data. Once we understand these concepts we will create our spectrogram images of the yes/no dataset to be used in the computer vision model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the dataset\n",
        "\n",
        "Here we import the packages and create a `load_audio` and `load_audio_files` function to load audio files from a specified path into a dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import tensorflow as tf\n",
        "import tensorflow_io as tfio\n",
        "import IPython.display as ipd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_audio(file_path):\n",
        "    audio_binary = tf.io.read_file(file_path)\n",
        "    audio, sample_rate = tf.audio.decode_wav(audio_binary)\n",
        "    waveform = tf.squeeze(audio, axis=-1)\n",
        "    return waveform, sample_rate\n",
        "\n",
        "def load_audio_files(path: str, label:str):\n",
        "\n",
        "    dataset = []\n",
        "    walker = sorted(str(p) for p in Path(path).glob(f'*.wav'))\n",
        "\n",
        "    for i, file_path in enumerate(walker):\n",
        "        path, filename = os.path.split(file_path)\n",
        "        speaker, _ = os.path.splitext(filename)\n",
        "        speaker_id, utterance_number = speaker.split(\"_nohash_\")\n",
        "        utterance_number = int(utterance_number)\n",
        "    \n",
        "        # Load audio\n",
        "        waveform, sample_rate = load_audio(file_path)\n",
        "        dataset.append([waveform, sample_rate, label, speaker_id, utterance_number])\n",
        "        \n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Call the `load_audio_files` function for each class we are going to use, then print the length of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainset_speechcommands_yes = load_audio_files('./data/yes', 'yes')\n",
        "trainset_speechcommands_no = load_audio_files('./data/no', 'no')\n",
        "\n",
        "print(f'Length of yes dataset: {len(trainset_speechcommands_yes)}')\n",
        "print(f'Length of no dataset: {len(trainset_speechcommands_no)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Now let's grab an example item from each dataset. We can see the waveform, sample_rate, label, and id."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "yes_waveform = trainset_speechcommands_yes[0][0]\n",
        "yes_sample_rate = trainset_speechcommands_yes[0][1]\n",
        "print(f'Yes Waveform: {yes_waveform}')\n",
        "print(f'Yes Sample Rate: {yes_sample_rate}')\n",
        "print(f'Yes Label: {trainset_speechcommands_yes[0][2]}')\n",
        "print(f'Yes ID: {trainset_speechcommands_yes[0][3]}')\n",
        "\n",
        "no_waveform = trainset_speechcommands_no[0][0]\n",
        "no_sample_rate = trainset_speechcommands_no[0][1]\n",
        "print(f'No Waveform: {no_waveform}')\n",
        "print(f'No Sample Rate: {no_sample_rate}')\n",
        "print(f'No Label: {trainset_speechcommands_no[0][2]}')\n",
        "print(f'No ID: {trainset_speechcommands_no[0][3]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transform and visualize\n",
        "\n",
        "Our data is ready! Let's breakdown some of the audio transforms and visualize them to better understand what they are and what they tell us about the data.\n",
        "\n",
        "\n",
        "### Waveform\n",
        "\n",
        "The waveform is generated by taking the sample rate and amplitude and representing the signal visually. This signal can be represented as a `waveform` which is the `signal` representation over time in a graphical format. The audio can be recorded in different `channels`. For example stereo recordings have 2 channels, right and left."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "timescale = np.arange(yes_waveform.shape[0])\n",
        "plt.figsize=(12, 8)\n",
        "plt.plot(timescale, yes_waveform.numpy())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Spectrogram\n",
        "\n",
        "Next we will look at the `Spectrogram`. What is a spectrogram anyway?! A spectrogram allows you to visualize the amplitude as a function of frequency and time in the form of an image, where the 'x' axis represents time, the 'y' axis represents frequency, and the color represents the amplitude. This image is what we will use for our computer vision classification on our audio files. \n",
        "\n",
        "Here we look at two different ways to create the spectrogram from the waveform. First we want to make our waveforms all equal lenths so we will pad them with zeros. Then we apply to transforms [tf.signal.stft](https://www.tensorflow.org/api_docs/python/tf/signal/stft) and [tfio.audio.spectrogram](https://www.tensorflow.org/io/api_docs/python/tfio/audio/spectrogram?hl=da).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        " def get_spectrogram(waveform):\n",
        "    \n",
        "    frame_length = 255\n",
        "    frame_step = 128\n",
        "    # Padding for files with less than 16000 samples\n",
        "    zero_padding = tf.zeros([16000] - tf.shape(waveform), dtype=tf.float32)\n",
        "\n",
        "    # Concatenate audio with padding so that all audio clips will be of the same length\n",
        "    waveform = tf.cast(waveform, tf.float32)\n",
        "    equal_length_waveform = tf.concat([waveform, zero_padding], 0)\n",
        "    \n",
        "    # Option 1: Use tfio to get the spectrogram\n",
        "    spect = tfio.audio.spectrogram(input=equal_length_waveform, nfft=frame_length, window=frame_length, stride=frame_step)\n",
        "    \n",
        "    # Option 2: Use tf.signal processing to get the Short-time Fourier transform (stft)\n",
        "    spectrogram = tf.signal.stft(equal_length_waveform, frame_length=frame_length, frame_step=frame_step)\n",
        "    spectrogram = tf.abs(spectrogram)\n",
        "\n",
        "    return spectrogram, spect\n",
        "\n",
        "## This funciton has some code from https://www.tensorflow.org/tutorials/audio/simple_audio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Lets listen to an example and then create a methond `plot_spectorgram` to display the two spectorgrams created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spectrogram, spect = get_spectrogram(yes_waveform)\n",
        "\n",
        "print('Label:', 'yes')\n",
        "print('Waveform shape:', yes_waveform.shape)\n",
        "print('Spectrogram shape:', spectrogram.shape)\n",
        "print('Spect shape:', spect.shape)\n",
        "print('Audio playback')\n",
        "ipd.Audio(yes_waveform.numpy(), rate=16000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_spectrogram(spectrogram, ax, title):\n",
        "    # Convert to frequencies to log scale and transpose so that the time is\n",
        "    # represented in the x-axis (columns).\n",
        "    log_spec = np.log(spectrogram.T)\n",
        "    height = log_spec.shape[0]\n",
        "    width = log_spec.shape[1]\n",
        "    X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n",
        "    Y = range(height)\n",
        "    ax.pcolormesh(X, Y, log_spec)\n",
        "    ax.set_xlim([0, 16000])\n",
        "    ax.set_title(title)\n",
        "    \n",
        "fig, ax = plt.subplots()  \n",
        "plot_spectrogram(spectrogram.numpy(), ax, 'Spectrogram 1')\n",
        "plt.show()\n",
        "\n",
        "fig, ax = plt.subplots()  \n",
        "plot_spectrogram(spect.numpy(), ax, 'Spectrogram 2')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save the spectrogram as an image\n",
        "\n",
        "We have broken down some of the ways to understand our audio data and different transformations we can use on our data. Now lets create the images we will use for classification. \n",
        "\n",
        "Below is a function to create the Spectrogram image for classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_images(dataset, label_dir):\n",
        "    # make directory\n",
        "    test_directory = f'./data/test/{label_dir}/'\n",
        "    train_directory = f'./data/train/{label_dir}/'\n",
        "\n",
        "    os.makedirs(test_directory, mode=0o777, exist_ok=True)\n",
        "    os.makedirs(train_directory, mode=0o777, exist_ok=True)\n",
        "    \n",
        "    for i, data in enumerate(dataset):\n",
        "\n",
        "        waveform = data[0]\n",
        "        spectrogram, spect = get_spectrogram(waveform)\n",
        "\n",
        "        # Split test and train images by 30%\n",
        "        if i % 3 == 0:\n",
        "            plt.imsave(f'./data/test/{label_dir}/spec_img{i}.png', spectrogram.numpy(), cmap='gray')\n",
        "        else:\n",
        "            plt.imsave(f'./data/train/{label_dir}/spec_img{i}.png', spectrogram.numpy(), cmap='gray')\n",
        "\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "create_images(trainset_speechcommands_yes, 'yes')\n",
        "create_images(trainset_speechcommands_no, 'no')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now have our audio as spectrogram images and are ready to build the model!"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "azureml_py38_PT_and_TF"
    },
    "kernelspec": {
      "display_name": "Python 3.10.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
